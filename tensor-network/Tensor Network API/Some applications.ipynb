{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5edc8495",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensornetwork as tn\n",
    "tn.set_default_backend(\"tensorflow\")\n",
    "class TNLayer(tf.keras.layers.Layer):\n",
    "\n",
    "  def __init__(self):\n",
    "    super(TNLayer, self).__init__()\n",
    "    # Create the variables for the layer.\n",
    "    self.a_var = tf.Variable(tf.random.normal(shape=(32, 32, 2),\n",
    "                                              stddev=1.0/32.0),\n",
    "                             name=\"a\", trainable=True)\n",
    "    self.b_var = tf.Variable(tf.random.normal(shape=(32, 32, 2),\n",
    "                                              stddev=1.0/32.0),\n",
    "                             name=\"b\", trainable=True)\n",
    "    self.bias = tf.Variable(tf.zeros(shape=(32, 32)),\n",
    "                            name=\"bias\", trainable=True)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    # Define the contraction.\n",
    "    # We break it out so we can parallelize a batch using\n",
    "    # tf.vectorized_map (see below).\n",
    "    def f(input_vec, a_var, b_var, bias_var):\n",
    "      # Reshape to a matrix instead of a vector.\n",
    "      input_vec = tf.reshape(input_vec, (32, 32))\n",
    "\n",
    "      # Now we create the network.\n",
    "      a = tn.Node(a_var)\n",
    "      b = tn.Node(b_var)\n",
    "      x_node = tn.Node(input_vec)\n",
    "      a[1] ^ x_node[0]\n",
    "      b[1] ^ x_node[1]\n",
    "      a[2] ^ b[2]\n",
    "\n",
    "      # The TN should now look like this\n",
    "      #   |     |\n",
    "      #   a --- b\n",
    "      #    \\   /\n",
    "      #      x\n",
    "\n",
    "      # Now we begin the contraction.\n",
    "      c = a @ x_node\n",
    "      result = (c @ b).tensor\n",
    "\n",
    "      # To make the code shorter, we also could've used Ncon.\n",
    "      # The above few lines of code is the same as this:\n",
    "      # result = tn.ncon([x, a_var, b_var], [[1, 2], [-1, 1, 3], [-2, 2, 3]])\n",
    "\n",
    "      # Finally, add bias.\n",
    "      return result + bias_var\n",
    "\n",
    "    # To deal with a batch of items, we can use the tf.vectorized_map\n",
    "    # function.\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/vectorized_map\n",
    "    result = tf.vectorized_map(\n",
    "        lambda vec: f(vec, self.a_var, self.b_var, self.bias), inputs)\n",
    "    return tf.nn.relu(tf.reshape(result, (-1, 1024)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08a5659c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 1024)              3072      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 1025      \n",
      "=================================================================\n",
      "Total params: 1,053,697\n",
      "Trainable params: 1,053,697\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Dense = tf.keras.layers.Dense\n",
    "fc_model = tf.keras.Sequential(\n",
    "    [\n",
    "     tf.keras.Input(shape=(2,)),\n",
    "     Dense(1024, activation=tf.nn.relu),\n",
    "     Dense(1024, activation=tf.nn.relu),\n",
    "     Dense(1, activation=None)])\n",
    "fc_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf50d8bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 1024)              3072      \n",
      "_________________________________________________________________\n",
      "tn_layer (TNLayer)           (None, 1024)              5120      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 1025      \n",
      "=================================================================\n",
      "Total params: 9,217\n",
      "Trainable params: 9,217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tn_model = tf.keras.Sequential(\n",
    "    [\n",
    "     tf.keras.Input(shape=(2,)),\n",
    "     Dense(1024, activation=tf.nn.relu),\n",
    "     # Here, we replace the dense layer with our MPS.\n",
    "     TNLayer(),\n",
    "     Dense(1, activation=None)])\n",
    "tn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62441e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running one_edge_at_a_time\n",
      "57.7 ms ± 2.84 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Running use_cotract_between\n",
      "3.3 ms ± 451 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def one_edge_at_a_time(a, b):\n",
    "  node1 = tn.Node(a)\n",
    "  node2 = tn.Node(b)\n",
    "  edge1 = node1[0] ^ node2[0]\n",
    "  edge2 = node1[1] ^ node2[1]\n",
    "  tn.contract(edge1)\n",
    "  result = tn.contract(edge2)\n",
    "  return result.tensor\n",
    "\n",
    "def use_contract_between(a, b):\n",
    "  node1 = tn.Node(a)\n",
    "  node2 = tn.Node(b)\n",
    "  node1[0] ^ node2[0]\n",
    "  node1[1] ^ node2[1]\n",
    "  # This is the same as \n",
    "  # tn.contract_between(node1, node2)\n",
    "  result = node1 @ node2\n",
    "  return result.tensor\n",
    "\n",
    "a = np.ones((1000, 1000))\n",
    "b = np.ones((1000, 1000))\n",
    "print(\"Running one_edge_at_a_time\")\n",
    "%timeit one_edge_at_a_time(a, b)\n",
    "print(\"Running use_cotract_between\")\n",
    "%timeit use_contract_between(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3798e3aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(64.0, shape=(), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "# Here, we will contract the following shaped network.\n",
    "# O - O\n",
    "# | X |\n",
    "# O - O\n",
    "a = tn.Node(np.ones((2, 2, 2)))\n",
    "b = tn.Node(np.ones((2, 2, 2)))\n",
    "c = tn.Node(np.ones((2, 2, 2)))\n",
    "d = tn.Node(np.ones((2, 2, 2)))\n",
    "# Make the network fully connected.\n",
    "a[0] ^ b[0]\n",
    "a[1] ^ c[1]\n",
    "a[2] ^ d[2]\n",
    "b[1] ^ d[1]\n",
    "b[2] ^ c[2]\n",
    "c[0] ^ d[0]\n",
    "# We are using the \"greedy\" contraction algorithm.\n",
    "# Other algorithms we support include \"optimal\" and \"branch\".\n",
    "\n",
    "# Finding the optimial contraction order in the general case is NP-Hard,\n",
    "# so there is no single algorithm that will work for every tensor network.\n",
    "# However, there are certain kinds of networks that have nice properties that\n",
    "# we can expliot to making finding a good contraction order easier.\n",
    "# These types of contraction algorithms are in developement, and we welcome \n",
    "# PRs!\n",
    "\n",
    "# `tn.reachable` will do a BFS to get all of the nodes reachable from a given\n",
    "# node or set of nodes.\n",
    "# nodes = {a, b, c, d}\n",
    "nodes = tn.reachable(a)\n",
    "result = tn.contractors.greedy(nodes)\n",
    "print(result.tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16e72022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float64, numpy=64.0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To make connecting a network a little less verbose, we have included\n",
    "# the NCon API aswell.\n",
    "\n",
    "# This example is the same as above.\n",
    "ones = np.ones((2, 2, 2))\n",
    "tn.ncon([ones, ones, ones, ones], \n",
    "        [[1, 2, 4], \n",
    "         [1, 3, 5], \n",
    "         [2, 3, 6],\n",
    "         [4, 5, 6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "282b7a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate([np.random.randn(20, 2) + np.array([3, 3]),\n",
    "                    np.random.randn(20, 2) + np.array([-3, -3]),\n",
    "                    np.random.randn(20, 2) + np.array([-3, 3]),\n",
    "                    np.random.randn(20, 2) + np.array([3, -3])])\n",
    "\n",
    "Y = np.concatenate([np.ones((40)), -np.ones((40))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29479008",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float64, numpy=\n",
       "array([[2., 2.],\n",
       "       [2., 2.]])>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To specify dangling edges, simply use a negative number on that index.\n",
    "\n",
    "ones = np.ones((2, 2))\n",
    "tn.ncon([ones, ones], [[-1, 1], [1, -2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bee9710c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make the singular values very apparent, we will just take the SVD of a\n",
    "# diagonal matrix.\n",
    "diagonal_array = np.array([[2.0, 0.0, 0.0],\n",
    "                           [0.0, 2.5, 0.0],\n",
    "                           [0.0, 0.0, 1.5]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef9782b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U node\n",
      "tf.Tensor(\n",
      "[[0.         1.41421356 0.        ]\n",
      " [1.58113883 0.         0.        ]\n",
      " [0.         0.         1.22474487]], shape=(3, 3), dtype=float64)\n",
      "\n",
      "V* node\n",
      "tf.Tensor(\n",
      "[[0.         1.58113883 0.        ]\n",
      " [1.41421356 0.         0.        ]\n",
      " [0.         0.         1.22474487]], shape=(3, 3), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "# First, we will go over the simple split_node method.\n",
    "a = tn.Node(diagonal_array)\n",
    "u, vh, _ = tn.split_node(\n",
    "    a, left_edges=[a[0]], right_edges=[a[1]])\n",
    "print(\"U node\")\n",
    "print(u.tensor)\n",
    "print()\n",
    "print(\"V* node\")\n",
    "print(vh.tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce7525d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contraction of U and V*:\n",
      "tf.Tensor(\n",
      "[[2.  0.  0. ]\n",
      " [0.  2.5 0. ]\n",
      " [0.  0.  1.5]], shape=(3, 3), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "# Now, we can contract u and vh to get back our original tensor!\n",
    "print(\"Contraction of U and V*:\")\n",
    "print((u @ vh).tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0b24a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[2.  0.  0. ]\n",
      " [0.  2.5 0. ]\n",
      " [0.  0.  0. ]], shape=(3, 3), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "# We can also drop the lowest singular values in 2 ways, \n",
    "# 1. By setting max_singular_values. This is the maximum number of the original\n",
    "# singular values that we want to keep.\n",
    "a = tn.Node(diagonal_array)\n",
    "u, vh, truncation_error = tn.split_node(\n",
    "    a, left_edges=[a[0]], right_edges=[a[1]], max_singular_values=2)\n",
    "# Notice how the two largest singular values (2.0 and 2.5) remain\n",
    "# but the smallest singular value (1.5) is removed.\n",
    "print((u @ vh).tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a13f34c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(10.0, shape=(), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensornetwork as tn\n",
    "\n",
    "# Create the nodes\n",
    "a = tn.Node(np.ones((10,))) \n",
    "b = tn.Node(np.ones((10,)))\n",
    "edge = a[0] ^ b[0] # Equal to tn.connect(a[0], b[0])\n",
    "final_node = tn.contract(edge)\n",
    "print(final_node.tensor) # Should print 10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45171374",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
